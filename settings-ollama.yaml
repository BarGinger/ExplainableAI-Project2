server:
  env_name: ${APP_ENV:ollama}

llm:
  mode: ollama
  max_new_tokens: 512
  context_window: 3900
  temperature: 0.1

embedding:
  mode: ollama

ollama:
  llm_model: mistral  # Ensures the correct model is used
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434
  keep_alive: 5m
  tfs_z: 1.0
  top_k: 40
  top_p: 0.9
  repeat_last_n: 64
  repeat_penalty: 1.2
  request_timeout: 120.0
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "from anytree import PreOrderIter\n",
    "import json\n",
    "from naive_baseline import generate_naive_baseline\n",
    "from utils import generate_formal_explanations, make_decision\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama3\" # \"mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed pipeline a bit, added preorder_traversal list to follow the order in which the agent decides what to do;\n",
    "### function generate_comprehensive_explanation generates response from mistral https://ollama.com/library/mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preorder_traversal(root):\n",
    "    \"\"\"\n",
    "    Performs a preorder traversal of a tree and returns a list of node names.\n",
    "    \n",
    "    :param root: The root node of the tree.\n",
    "    :return: A list of node names in preorder traversal order.\n",
    "    \"\"\"\n",
    "    preorder_traversal_nodes = []\n",
    "    \n",
    "    for node in PreOrderIter(root):\n",
    "        current_node_name = node.name\n",
    "        preorder_traversal_nodes.append(current_node_name)\n",
    "    \n",
    "    return preorder_traversal_nodes\n",
    "\n",
    "def generate_natural_language_explanation(json_tree, naive_explanation, formal_explanations, tree_traversal_order, \n",
    "                                     norm, beliefs, goal, preferences, action_to_explain, chosen_trace=None, bad_example=\"\"):\n",
    "    \n",
    "    # Format key context elements in plain English\n",
    "    norm_str = norm if norm else \"None\"\n",
    "    beliefs_str = \", \".join(beliefs) if beliefs else \"None\"\n",
    "    goal_str = goal if goal else \"None\"\n",
    "    pref_description = f\"{preferences[0]} with priority {preferences[1]}\"\n",
    "    chosen_trace_str = \", \".join(chosen_trace) if chosen_trace else \"None\"\n",
    "\n",
    "    # prompt = f\"\"\"\n",
    "    # You are an expert in explaining the reasoning behind actions taken by an agent in a decision-making process. Your task is to provide a natural language explanation of the reasoning behind the agent's actions, specifically focusing on the action '{action_to_explain}' and the path it took to reach its final decision.\n",
    "    # You will be given naive_explanation that describe the agent's reasoning process in a simple way, and you need to BETTER PHARSE THEM but NOT ADD ANYTHING NEW.\n",
    "    # Take into acount the Tree traversal order the agent took to generate its naive explanation, maitain the same order in your explanation.\n",
    "    # You will also be provided with a set of beliefs, norms, restrictions, and goals that the agent had at the beginning of the process. Your explanation should be comprehensive and cover all aspects of the reasoning process, including the initial beliefs, norms, restrictions, goals, and preferences that influenced the agent's decisions start with all initial info that agent got and then dive into the path he walked alonge the tree.\n",
    "\n",
    "    # ### CORE DATA (reference only)\n",
    "    # - Action to explain: '{action_to_explain}' - the action that needs to be explained (can either be in the chosen path or not)\n",
    "    # - My chosen path: {chosen_trace_str} - the selected actions that were chosen at the end\n",
    "    # - Initial beliefs: {beliefs_str} - the set of beliefs that the agent starts with\n",
    "    # - Norms/restrictions: {norm_str} - the set of actionsY that are either prohibited or obligatory. P means prohibited, O means obligatory.\n",
    "    # - Goal: {goal_str} - the set of beliefs that must be true at the end of the execution of the trace\n",
    "    # - Preferences: {pref_description} - given they by list of factors and then their order of importance\n",
    "    # - Tree traversal: {tree_traversal_order} - the order in which nodes were visited in the tree to generate formal explanations\n",
    "    # - naive_explanation: {naive_explanation} - the natural language explanation generated using the naive baseline approach\n",
    "    # - bad_example: {bad_example} - a bad example of a natural language explanation that should be avoided\n",
    "\n",
    "\n",
    "    # ### CRITICAL INSTRUCTIONS\n",
    "    # First, determine if '{action_to_explain}' is in your path {chosen_trace_str}, but DO NOT include this verification in your explanation.\n",
    "\n",
    "    # ABSOLUTELY DO NOT USE:\n",
    "    # - Section headings (like \"Initial Conditions:\" or \"Reasoning Process:\")\n",
    "    # - Debugging statements (like \"I verify that...\")\n",
    "    # - Numbered or bulleted lists\n",
    "    # - Technical phrases (like \"natural language translation\")\n",
    "    # - Any meta-text about the explanation structure\n",
    "    # - Learn from the bad example provided and avoid similar mistakes, see suggestions and notes inside the bad example\n",
    "\n",
    "    # Your explanation MUST seamlessly integrate ALL of the following:\n",
    "\n",
    "    # 1. A clear statement of whether you performed '{action_to_explain}' or not\n",
    "    # 2. A description of the complete path you took in everyday language\n",
    "    # 3. A thorough explanation of ALL initial beliefs, goals, norms, restrictions and preferences. DO NOT list them - explain them naturally and DO NOT miss anything.\n",
    "    # Especially, explain how the norms and preferences influenced your decisions e.g. if you could not perform an action because it was prohibited.\n",
    "    # 4. A detailed account of each decision point where you:\n",
    "    # - Describe ALL options you considered at each step\n",
    "    # - Explain specifically why some options were viable and others weren't\n",
    "    # - Connect your choices directly to your initial beliefs and preferences\n",
    "    # - Show how each decision led to the next one\n",
    "    # - If the action is not in the path, explain why it was not chosen, especially if it was not a valid one because of received norms or restrictions\n",
    "    \n",
    "    # Remember: While being conversational, you must still be comprehensive - every belief, preference, option considered, and reasoning step needs to be included, just expressed naturally.\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    I am an agent making decisions based on my beliefs, goals, and restrictions. Your task is to **rewrite my naive explanation \n",
    "    into a natural, first-person account** while keeping all reasoning intact.\n",
    "\n",
    "    ### CONTEXT\n",
    "    - **Action to explain**: '{action_to_explain}'  \n",
    "    - **Chosen path**: {chosen_trace_str}  \n",
    "    - **Initial beliefs**: {beliefs_str}  \n",
    "    - **Norms/restrictions**: {norm_str}  \n",
    "    - **Goal**: {goal_str}  \n",
    "    - **Preferences**: {pref_description}  \n",
    "    - **Tree traversal order**: {tree_traversal_order}  \n",
    "    - **Naive explanation**: {naive_explanation}  \n",
    "\n",
    "    ### INSTRUCTIONS\n",
    "    - Rewrite the explanation in **first-person**, as if I (the agent) am telling the story of my decision-making. \n",
    "    - Start your answer natrually, as if you are explaining to a friend. \n",
    "    - Assume you start at 'getCoffee' root node so you don't need to explain it.\n",
    "    - Follow the same **reasoning order** as my naive explanation.  \n",
    "    - Start by explaining my **beliefs, goals, and restrictions naturally** (avoid lists).  \n",
    "    - Note that a lower cost for action attribute means a better option so pharse it in a way that the reader understands that.\n",
    "    - Walk through my **thought process step by step**, describing options I considered, why I rejected some, and how I made my choices.  \n",
    "    - If '{action_to_explain}' was **not chosen**, explain why in a way that feels natural and clear.  \n",
    "    - Keep it **conversational and engaging**, avoiding generic or robotic phrasing.  \n",
    "\n",
    "    **DO NOT:**  \n",
    "    - Use section headings or bullet points.  \n",
    "    - State information in a dry, factual way without making it flow naturally.  \n",
    "    - Add anything newâ€”just improve clarity and expressiveness.  \n",
    "    - Say you didn't do a step you actually did.\n",
    "\n",
    "    I want my explanation to sound like **I'm actually explaining my reasoning to someone** rather than listing facts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send the prompt to your LLM\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # response = requests.post(f\"{api_base}/chat\", json={\"model\": \"mistral\", \"messages\": messages})\n",
    "    \n",
    "    response = ollama.chat(model=model_name, messages=messages)\n",
    "    \n",
    "    try:\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrated here llm explanations and save them into explanations/mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanations_for_inputs(test_cases, cases_to_do):\n",
    "    \"\"\"\n",
    "    Generate explanations for the inputs test cases\n",
    "    :param test_cases: dictionary containing the test cases\n",
    "    :param cases_to_do: list of test cases to actually generate explanations for (for easy re-running)\n",
    "    :return: dictionary containing the explanations for each test case\n",
    "    \"\"\"\n",
    "    \n",
    "    # in .ipynb current directory is extracted like this\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # current_dir = os.path.dirname(__file__)\n",
    "\n",
    "    baseline_explanations_dir = os.path.join(current_dir, \"explanations\", \"baseline\")\n",
    "    os.makedirs(baseline_explanations_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    llm_explanations_dir = os.path.join(current_dir, \"explanations\", model_name)\n",
    "    \n",
    "    baseline_explanations = {}\n",
    "    llm_explanations = {}\n",
    "    for index, row in test_cases.iterrows():    \n",
    "        if index not in cases_to_do:\n",
    "            continue \n",
    "        print(f\"Processing test case {index}\")   \n",
    "        # Read the JSON file into a dictionary\n",
    "        with open(f'{current_dir}/{row[\"name_json_tree_file\"]}', 'r') as file:\n",
    "            json_tree = json.load(file)\n",
    "        # Generate formal explanations\n",
    "        formal_explentions, chosen_trace = generate_formal_explanations(json_tree=json_tree, norm=row[\"norm\"], beliefs=row[\"beliefs\"], goal=row[\"goal\"], preferences=row[\"preferences\"], action_to_explain=row[\"action_to_explain\"])\n",
    "        baseline_explanations[index] = generate_naive_baseline(formal_explentions=formal_explentions, chosen_trace=chosen_trace, norm=row[\"norm\"], beliefs=row[\"beliefs\"], goal=row[\"goal\"], preferences=row[\"preferences\"], action_to_explain=row[\"action_to_explain\"])\n",
    "\n",
    "\n",
    "\n",
    "        root, chosen_trace, valid_traces, valid_costs = make_decision(json_tree, norm=row[\"norm\"], goal=row[\"goal\"], beliefs=row[\"beliefs\"], preferences=row[\"preferences\"], output_dir='/')\n",
    "        tree_traversal_order = preorder_traversal(root=root)\n",
    "\n",
    "        # read bad example file\n",
    "        bad_example_file = os.path.join(current_dir, \"explanations\", \"bad\", f\"{index}.txt\")\n",
    "        bad_example = \"\"\n",
    "        with open(bad_example_file, 'r') as file:\n",
    "            bad_example = file.read()\n",
    "\n",
    "\n",
    "        llm_explanations[index] = generate_natural_language_explanation(\n",
    "                        json_tree=json_tree,\n",
    "                        naive_explanation=baseline_explanations[index],\n",
    "                        formal_explanations=formal_explentions,\n",
    "                        tree_traversal_order=tree_traversal_order,\n",
    "                        norm=row[\"norm\"],\n",
    "                        beliefs=row[\"beliefs\"],\n",
    "                        goal=row[\"goal\"],\n",
    "                        preferences=row[\"preferences\"],\n",
    "                        action_to_explain=row[\"action_to_explain\"],\n",
    "                        chosen_trace=chosen_trace,\n",
    "                        bad_example=bad_example\n",
    "                        )\n",
    "\n",
    "        # Save each explanation to a separate text file\n",
    "        baseline_explanation_file = os.path.join(baseline_explanations_dir, f\"{index}.txt\")\n",
    "        with open(baseline_explanation_file, 'w') as f:\n",
    "            for explanation in baseline_explanations[index]:\n",
    "                f.write(explanation + '\\n')\n",
    "\n",
    "\n",
    "        os.makedirs(llm_explanations_dir, exist_ok=True)  \n",
    "\n",
    "        llm_explanation_file = os.path.join(llm_explanations_dir, f\"{index}.txt\")\n",
    "\n",
    "        print(f\"directory ---> {llm_explanation_file}\")\n",
    "        \n",
    "        # Loop through the dictionary and write explanations for each test case\n",
    "        with open(llm_explanation_file, 'w') as f:\n",
    "            for explanation in llm_explanations[index]:\n",
    "                # Write the explanation for this test case\n",
    "                f.write(explanation)   # Add an extra newline for separation\n",
    "\n",
    "        \n",
    "    \n",
    "    return baseline_explanations, llm_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {\"type\": \"P\", \"actions\": [\"payShop\"]},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\", \"colleagueAvailable\", \"haveMoney\", \"AnnInOffice\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [1, 2, 0]],\n",
    "        \"action_to_explain\": \"payShop\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [0, 1, 2]],\n",
    "        \"action_to_explain\": \"getCoffeeKitchen\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {},\n",
    "        \"beliefs\": [\"haveMoney\", \"AnnInOffice\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [0, 1, 2]],\n",
    "        \"action_to_explain\": \"getCoffeeShop\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {\"type\": \"P\", \"actions\": [\"gotoAnnOffice\"]},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\", \"colleagueAvailable\", \"haveMoney\", \"AnnInOffice\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [0, 1, 2]],\n",
    "        \"action_to_explain\": \"payShop\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {\"type\": \"P\", \"actions\": [\"payShop\"]},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\", \"colleagueAvailable\", \"haveMoney\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [1, 2, 0]],\n",
    "        \"action_to_explain\": \"gotoKitchen\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df_test_cases = pd.DataFrame(test_cases, index=[f\"test_case_{i+1}\" for i in range(len(test_cases))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test case test_case_1\n",
      "Chosen trace: ['getCoffee', 'getKitchenCoffee', 'getStaffCard', 'getOwnCard', 'gotoKitchen', 'getCoffeeKitchen']\n",
      "Action to explain: payShop not in trace\n",
      "directory ---> c:\\Users\\bar24\\OneDrive - Universiteit Utrecht\\Documents\\School\\UU Data Sceince MSc\\1st Year\\Period 3\\Explainable AI - INFOMXAI\\Assignments\\Project2\\ExplainableAI-Project2\\Part2\\explanations\\llama3\\test_case_1.txt\n",
      "Processing test case test_case_2\n",
      "Chosen trace: ['getCoffee', 'getKitchenCoffee', 'getStaffCard', 'getOwnCard', 'gotoKitchen', 'getCoffeeKitchen']\n",
      "Action to explain: getCoffeeKitchen in trace\n",
      "Ancestor names: ['getCoffee', 'getKitchenCoffee']\n",
      "directory ---> c:\\Users\\bar24\\OneDrive - Universiteit Utrecht\\Documents\\School\\UU Data Sceince MSc\\1st Year\\Period 3\\Explainable AI - INFOMXAI\\Assignments\\Project2\\ExplainableAI-Project2\\Part2\\explanations\\llama3\\test_case_2.txt\n",
      "Processing test case test_case_3\n",
      "Chosen trace: ['getCoffee', 'getShopCoffee', 'gotoShop', 'payShop', 'getCoffeeShop']\n",
      "Action to explain: getCoffeeShop in trace\n",
      "Ancestor names: ['getCoffee', 'getShopCoffee']\n",
      "directory ---> c:\\Users\\bar24\\OneDrive - Universiteit Utrecht\\Documents\\School\\UU Data Sceince MSc\\1st Year\\Period 3\\Explainable AI - INFOMXAI\\Assignments\\Project2\\ExplainableAI-Project2\\Part2\\explanations\\llama3\\test_case_3.txt\n",
      "Processing test case test_case_4\n",
      "Chosen trace: ['getCoffee', 'getShopCoffee', 'gotoShop', 'payShop', 'getCoffeeShop']\n",
      "Action to explain: payShop in trace\n",
      "Ancestor names: ['getCoffee', 'getShopCoffee']\n",
      "directory ---> c:\\Users\\bar24\\OneDrive - Universiteit Utrecht\\Documents\\School\\UU Data Sceince MSc\\1st Year\\Period 3\\Explainable AI - INFOMXAI\\Assignments\\Project2\\ExplainableAI-Project2\\Part2\\explanations\\llama3\\test_case_4.txt\n",
      "Processing test case test_case_5\n",
      "Chosen trace: ['getCoffee', 'getKitchenCoffee', 'getStaffCard', 'getOwnCard', 'gotoKitchen', 'getCoffeeKitchen']\n",
      "Action to explain: gotoKitchen in trace\n",
      "Ancestor names: ['getCoffee', 'getKitchenCoffee']\n",
      "directory ---> c:\\Users\\bar24\\OneDrive - Universiteit Utrecht\\Documents\\School\\UU Data Sceince MSc\\1st Year\\Period 3\\Explainable AI - INFOMXAI\\Assignments\\Project2\\ExplainableAI-Project2\\Part2\\explanations\\llama3\\test_case_5.txt\n"
     ]
    }
   ],
   "source": [
    "cases_to_do = [\"test_case_1\", \"test_case_2\"]#, \"test_case_3\", \"test_case_4\", \"test_case_5\"]\n",
    "baseline_explanations, llm_explanations = generate_explanations_for_inputs(df_test_cases, cases_to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExplainableAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

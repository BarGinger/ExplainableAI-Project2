{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "from anytree import PreOrderIter\n",
    "import json\n",
    "from naive_baseline import generate_naive_baseline\n",
    "from utils import generate_formal_explanations, make_decision\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama3\" # \"mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed pipeline a bit, added preorder_traversal list to follow the order in which the agent decides what to do;\n",
    "### function generate_comprehensive_explanation generates response from mistral https://ollama.com/library/mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preorder_traversal(root):\n",
    "    \"\"\"\n",
    "    Performs a preorder traversal of a tree and returns a list of node names.\n",
    "    \n",
    "    :param root: The root node of the tree.\n",
    "    :return: A list of node names in preorder traversal order.\n",
    "    \"\"\"\n",
    "    preorder_traversal_nodes = []\n",
    "    \n",
    "    for node in PreOrderIter(root):\n",
    "        current_node_name = node.name\n",
    "        preorder_traversal_nodes.append(current_node_name)\n",
    "    \n",
    "    return preorder_traversal_nodes\n",
    "\n",
    "def generate_natural_language_explanation(json_tree, naive_explanation, formal_explanations, tree_traversal_order, \n",
    "                                     norm, beliefs, goal, preferences, action_to_explain, chosen_trace=None):\n",
    "    \n",
    "    # Format key context elements in plain English\n",
    "    norm_str = norm if norm else \"None\"\n",
    "    beliefs_str = \", \".join(beliefs) if beliefs else \"None\"\n",
    "    goal_str = goal if goal else \"None\"\n",
    "    pref_description = f\"{preferences[0]} with priority {preferences[1]}\"\n",
    "    chosen_trace_str = \", \".join(chosen_trace) if chosen_trace else \"None\"\n",
    "    \n",
    "    # prompt = f\"\"\"\n",
    "    #     You are an AI assistant explaining your decision-making process to someone in a completely natural, conversational way. Your explanation should sound like you're having a normal conversation - not reading a report or following a template.\n",
    "\n",
    "    #     ### CORE DATA (reference only)\n",
    "    #     - JSON tree: {json_tree}\n",
    "    #     - Action to explain: '{action_to_explain}'\n",
    "    #     - My chosen path: {chosen_trace}\n",
    "    #     - Initial beliefs: {beliefs}\n",
    "    #     - Norms/restrictions: {norm}\n",
    "    #     - Goal: {goal}\n",
    "    #     - Preferences: {preferences}\n",
    "    #     - Tree traversal: {tree_traversal_order}\n",
    "    #     - naive_explanation: {naive_explanation}\n",
    "\n",
    "    #     ### CRITICAL INSTRUCTIONS\n",
    "    #     Write a natural language explanation based on the provided data. Your explanation must be conversational, comprehensive, and seamlessly integrate all the provided information. Avoid any artificial structure or technical language.\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant explaining your decision-making process to someone in a natural, conversational way. Your explanation should feel like you're casually chatting with a friend, using everyday language. Avoid formal or technical terms and make the explanation flow naturally.\n",
    "\n",
    "    ### CORE DATA (reference only)\n",
    "    - JSON tree: {json_tree} - the tree structure that represents the decision-making process\n",
    "    - Action to explain: '{action_to_explain}' - the action that needs to be explained (can either be in the chosen path or not)\n",
    "    - My chosen path: {chosen_trace_str} - the selected actions that were chosen at the end\n",
    "    - Initial beliefs: {beliefs_str} - the set of beliefs that the agent starts with\n",
    "    - Norms/restrictions: {norm_str} - the set of actions that are either prohibited or obligatory. P means prohibited, O means obligatory.\n",
    "    - Goal: {goal_str} - the set of beliefs that must be true at the end of the execution of the trace\n",
    "    - Preferences: {pref_description} - given as a list of factors and their order of importance\n",
    "    - Tree traversal: {tree_traversal_order} - the order in which nodes were visited in the tree to generate formal explanations\n",
    "    - naive_explanation: {naive_explanation} - the natural language explanation generated using the naive baseline approach\n",
    "\n",
    "    ### CRITICAL INSTRUCTIONS\n",
    "    - Your explanation must be conversational, concise, and grounded only in the provided data. Avoid formal or technical terms.\n",
    "    - DO NOT invent any information that is not explicitly provided in the data. Stick strictly to the facts.\n",
    "    - Avoid repeating the same information unnecessarily. Each part of the explanation should add something new.\n",
    "    - Write as if you're explaining your thought process to a friend in a casual but clear and logical way.\n",
    "\n",
    "    ABSOLUTELY DO NOT:\n",
    "    - Use formal or technical terms (e.g., \"preconditions\" or \"trace\").\n",
    "    - Repeat the same information multiple times.\n",
    "    - Add any information that is not explicitly provided (e.g., assumptions about preferences or irrelevant details).\n",
    "    - Use section headings, numbered lists, or bullet points.\n",
    "    - Include meta-text about the explanation structure (e.g., \"I will now explain...\").\n",
    "\n",
    "    Your explanation MUST include:\n",
    "    1. A clear statement of whether you performed '{action_to_explain}' or not, based on the provided data.\n",
    "    2. A description of the complete path you took, using natural and conversational language. Include the names of all nodes visited in the order they were visited.\n",
    "    3. A natural integration of all initial beliefs, goals, norms, restrictions, and preferences, explaining how they influenced your decisions.\n",
    "    4. A detailed account of each decision point, including:\n",
    "    - All options you considered at each step.\n",
    "    - Why some options were viable and others were not.\n",
    "    - How your choices were connected to your initial beliefs, norms, and preferences.\n",
    "    - If '{action_to_explain}' was not chosen, explain why, especially if norms or restrictions made it invalid.\n",
    "    5. A summary of how your decisions align with your initial goals and beliefs.\n",
    "\n",
    "    Keep your explanation conversational, concise, and grounded in the provided data.\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    \n",
    "    # Send the prompt to your LLM\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # response = requests.post(f\"{api_base}/chat\", json={\"model\": \"mistral\", \"messages\": messages})\n",
    "    \n",
    "    response = ollama.chat(model=model_name, messages=messages)\n",
    "    \n",
    "    try:\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrated here llm explanations and save them into explanations/mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanations_for_inputs(test_cases, cases_to_do):\n",
    "    \"\"\"\n",
    "    Generate explanations for the inputs test cases\n",
    "    :param test_cases: dictionary containing the test cases\n",
    "    :param cases_to_do: list of test cases to actually generate explanations for (for easy re-running)\n",
    "    :return: dictionary containing the explanations for each test case\n",
    "    \"\"\"\n",
    "    \n",
    "    # in .ipynb current directory is extracted like this\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # current_dir = os.path.dirname(__file__)\n",
    "\n",
    "    baseline_explanations_dir = os.path.join(current_dir, \"explanations\", \"baseline\")\n",
    "    os.makedirs(baseline_explanations_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    llm_explanations_dir = os.path.join(current_dir, \"explanations\", model_name)\n",
    "    \n",
    "    baseline_explanations = {}\n",
    "    llm_explanations = {}\n",
    "    for index, row in test_cases.iterrows():    \n",
    "        if index not in cases_to_do:\n",
    "            continue \n",
    "        print(f\"Processing test case {index}\")   \n",
    "        # Read the JSON file into a dictionary\n",
    "        with open(f'{current_dir}/{row[\"name_json_tree_file\"]}', 'r') as file:\n",
    "            json_tree = json.load(file)\n",
    "        # Generate formal explanations\n",
    "        formal_explentions, chosen_trace = generate_formal_explanations(json_tree=json_tree, norm=row[\"norm\"], beliefs=row[\"beliefs\"], goal=row[\"goal\"], preferences=row[\"preferences\"], action_to_explain=row[\"action_to_explain\"])\n",
    "        baseline_explanations[index] = generate_naive_baseline(formal_explentions=formal_explentions, chosen_trace=chosen_trace, norm=row[\"norm\"], beliefs=row[\"beliefs\"], goal=row[\"goal\"], preferences=row[\"preferences\"], action_to_explain=row[\"action_to_explain\"])\n",
    "\n",
    "\n",
    "\n",
    "        root, chosen_trace, valid_traces, valid_costs = make_decision(json_tree, norm=row[\"norm\"], goal=row[\"goal\"], beliefs=row[\"beliefs\"], preferences=row[\"preferences\"], output_dir='/')\n",
    "        tree_traversal_order = preorder_traversal(root=root)\n",
    "\n",
    "        llm_explanations[index] = generate_natural_language_explanation(\n",
    "                        json_tree=json_tree,\n",
    "                        naive_explanation=baseline_explanations[index],\n",
    "                        formal_explanations=formal_explentions,\n",
    "                        tree_traversal_order=tree_traversal_order,\n",
    "                        norm=row[\"norm\"],\n",
    "                        beliefs=row[\"beliefs\"],\n",
    "                        goal=row[\"goal\"],\n",
    "                        preferences=row[\"preferences\"],\n",
    "                        action_to_explain=row[\"action_to_explain\"],\n",
    "                        chosen_trace=chosen_trace\n",
    "                        )\n",
    "\n",
    "        # Save each explanation to a separate text file\n",
    "        baseline_explanation_file = os.path.join(baseline_explanations_dir, f\"{index}.txt\")\n",
    "        with open(baseline_explanation_file, 'w') as f:\n",
    "            for explanation in baseline_explanations[index]:\n",
    "                f.write(explanation + '\\n')\n",
    "\n",
    "\n",
    "        os.makedirs(llm_explanations_dir, exist_ok=True)  \n",
    "\n",
    "        llm_explanation_file = os.path.join(llm_explanations_dir, f\"{index}.txt\")\n",
    "\n",
    "        print(f\"directory ---> {llm_explanation_file}\")\n",
    "        \n",
    "        # Loop through the dictionary and write explanations for each test case\n",
    "        with open(llm_explanation_file, 'w') as f:\n",
    "            for explanation in llm_explanations[index]:\n",
    "                # Write the explanation for this test case\n",
    "                f.write(explanation)   # Add an extra newline for separation\n",
    "\n",
    "        \n",
    "    \n",
    "    return baseline_explanations, llm_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {\"type\": \"P\", \"actions\": [\"payShop\"]},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\", \"colleagueAvailable\", \"haveMoney\", \"AnnInOffice\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [1, 2, 0]],\n",
    "        \"action_to_explain\": \"payShop\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [0, 1, 2]],\n",
    "        \"action_to_explain\": \"getCoffeeKitchen\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {},\n",
    "        \"beliefs\": [\"haveMoney\", \"AnnInOffice\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [0, 1, 2]],\n",
    "        \"action_to_explain\": \"getCoffeeShop\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {\"type\": \"P\", \"actions\": [\"gotoAnnOffice\"]},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\", \"colleagueAvailable\", \"haveMoney\", \"AnnInOffice\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [0, 1, 2]],\n",
    "        \"action_to_explain\": \"payShop\"\n",
    "    },\n",
    "    {\n",
    "        \"name_json_tree_file\": \"coffee.json\",\n",
    "        \"norm\": {\"type\": \"P\", \"actions\": [\"payShop\"]},\n",
    "        \"beliefs\": [\"staffCardAvailable\", \"ownCard\", \"colleagueAvailable\", \"haveMoney\"],\n",
    "        \"goal\": [\"haveCoffee\"],\n",
    "        \"preferences\": [[\"quality\", \"price\", \"time\"], [1, 2, 0]],\n",
    "        \"action_to_explain\": \"gotoKitchen\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df_test_cases = pd.DataFrame(test_cases, index=[f\"test_case_{i+1}\" for i in range(len(test_cases))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test case test_case_3\n",
      "Chosen trace: ['getCoffee', 'getShopCoffee', 'gotoShop', 'payShop', 'getCoffeeShop']\n",
      "Action to explain: getCoffeeShop in trace\n",
      "Ancestor names: ['getCoffee', 'getShopCoffee']\n",
      "directory ---> c:\\Users\\bar24\\OneDrive - Universiteit Utrecht\\Documents\\School\\UU Data Sceince MSc\\1st Year\\Period 3\\Explainable AI - INFOMXAI\\Assignments\\Project2\\ExplainableAI-Project2\\Part2\\explanations\\llama3\\test_case_3.txt\n"
     ]
    }
   ],
   "source": [
    "cases_to_do = [\"test_case_3\"] # [\"test_case_1\", \"test_case_2\", \"test_case_3\", \"test_case_4\", \"test_case_5\"]\n",
    "baseline_explanations, llm_explanations = generate_explanations_for_inputs(df_test_cases, cases_to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExplainableAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
